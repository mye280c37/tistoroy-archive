2008ë…„ì— ì“°ì¸ ê¸€ì´ì§€ë§Œ MARLì˜ í° í‹€ì„ ì¡ê³  ì ì •ë¦¬í•´ë³´ì•˜ë‹¤.

[A Comprehensive Survey of Multiagent Reinforcement Learning](https://ieeexplore.ieee.org/document/4445757)

## Benefits and Challenges in MARL

### Benefits

- agentë¼ë¦¬ ê²½í—˜ì„ ê³µìœ í•˜ë©´ ë¹„ìŠ·í•œ taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²½ìš° ì„œë¡œì—ê²Œ ë„ì›€ì´ ëœë‹¤.

### Challenges

- ì°¨ì›ì˜ ì €ì£¼
- agentë¼ë¦¬ì˜ ìƒê´€ê´€ê³„ ë•Œë¬¸ì— ì„œë¡œ í•™ìŠµì— ë°©í•´ê°€ ë  ìˆ˜ ìˆë‹¤.
- agentë“¤ì´ ê³„ì† actionì„ ì·¨í•˜ê³  ì´ì— ë”°ë¼ environmentê°€ ë³€í•˜ê¸° ë•Œë¬¸ì— nonstationary ìƒíƒœê°€ ëœë‹¤.
	- environment ë¿ë§Œ ì•„ë‹ˆë¼ other agentsë„ ê³ ë ¤í•´ì•¼ í•¨


## MARL GOAL

<ol>
<li> Stability 
<div class="inbox">
    â¡ï¸ dynamic environments ìƒí™©ì—ì„œë„ stationary policyê°€ convergence ìƒíƒœì— ì´ë¥´ëŠ” ê²ƒ
</div>
</li>
<li> Adaptation
<div class="inbox">
    â¡ï¸  ë‹¤ë¥¸ agentsì˜ ì˜í–¥ì—ë„ ìì‹ ì˜ Performanceë¥¼ ìœ ì§€í•˜ê±°ë‚˜ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒ
</div>
</li>
</ol>

### Stability Property

<ol>
<li> equilibrium learning
<div class="inbox">
<ol>
    <li> converge to a coordinated equilibrium </li>
    <li> Nash equilibria </li>
    <li> stagewise convergenceì™€  Nash equilibria ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ ì •í™•í•˜ì§€ ì•Šë‹¤ëŠ” ë¹„íŒ [14] </li>
</ol>
</div>
</li>
<li>        
convergence
</li>
<li>
opponent independent
</li>

<div class="aside">
<div class="title">
ğŸ“Œ Nash equilibrium
</div>
<div class="contents">
 ë‚´ì‹œ ê· í˜•. ê²Œì„ì´ë¡ ì—ì„œ ê²½ìŸìì˜ ëŒ€ì‘ì— ë”°ë¼ ìµœì„ ì˜ ì„ íƒì„ í•˜ë©´ ì„œë¡œê°€ ìì‹ ì˜ ì„ íƒì„ ë°”ê¾¸ì§€ ì•ŠëŠ” ê· í˜•ìƒíƒœ.
  
 ìƒëŒ€ë°©ì´ í˜„ì¬ ì „ëµì„ ìœ ì§€í•œë‹¤ëŠ” ì „ì œ í•˜ì— ë‚˜ ìì‹ ë„ í˜„ì¬ ì „ëµì„ ë°”ê¿€ ìœ ì¸ì´ ì—†ëŠ” ìƒíƒœ
  
 ì–´ë–¤ agent aê°€ ë‹¤ë¥¸ policyë¥¼ ë“¤ê³ ì™€ë„ returnì„ ìµœëŒ€í™”í•  ìˆ˜ ì—†ëŠ” í‰í˜•ìƒíƒœ
  
 ì¶œì²˜ : <a href="https://ko.wikipedia.org/wiki/%EB%82%B4%EC%8B%9C_%EA%B7%A0%ED%98%95">https://ko.wikipedia.org/wiki/ë‚´ì‹œ_ê· í˜•</a>
 </div>
</div>
  
### The  need for Stability

- more amenable to analaysis and meaningful performance guarantees
- reduces the nonstationarity in the learning problem of other agents

### Adaptation Property

<ol>
<li> Rationality
    <div class="inbox">
    <ul>
    <li> converge to best response when the other agents remain stationary. </li>
    <li> [13], [56] ì°¸ê³  </li>
    </ul>
    </div>
</li>
<li> no-regret
<div class="inbox">
<ul>
    <li> at least as good as the return of any stationary strategy </li>
    <li> agentê°€ ë‹¤ë¥¸ agentì— ì˜í•´ â€œbeing exploitedâ€ ë˜ëŠ” ê²ƒì„ ë§‰ëŠ” ê²Œ ëª©ì  </li>
    <li> [57] ì°¸ê³  </li>
</ul>
</div>
</li>
<li> Targeted optimality/compatibility/safety
<div class="inbox">
<ul>
    <li>expressed in the form of average reward bounds</li>
</ul>
</div>
</li>
<li> openent-aware</li>
</ol>

### The need for Adaptation

- ì—¬ëŸ¬ agentë“¤ì´ í™˜ê²½ì— ìˆìŒìœ¼ë¡œì¨ ë°œìƒí•˜ëŠ” â€œthe dynamicsâ€ê°€ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ë‹¤

## Taxonomy of MARL Algorithms

- Stabilityì—ë§Œ ì§‘ì¤‘í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
    - ë‹¤ë¥¸ agentë“¤ì˜ í–‰ë™ì„ ì¸ì‹ X
    - independent
- Adaptationë„ ê³ ë ¤í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
    - need to be aware to some extent of their behavior

### Task ì¢…ë¥˜

- static (stateless)
- dynamic

### Agentë“¤ê°„ì˜ ê´€ê³„

[##_Image|kage@bc9ZDx/btr3dT4ppxz/4DYzscDDzvhFJp7p4lLztk/img.png|alignCenter|width="100%"|_##]

- fully cooperative
- fully competitive
- mixed

### the organization of the algorithms

[##_Image|kage@bYVbPa/btr3cbLxr7D/hWLySWHAPgM5kIF8PfffF1/img.png|alignCenter|width="100%"|_##]

- temproal-differene RL
- game theory
- direct policy search

### other taxonomy axes (ë‹¤ë¥¸ ë¶„ë¥˜ ê¸°ì¤€)

- Homogeneity vs. Heterogeneous
    - Homogeneity : ëª¨ë“  agentê°€ í•˜ë‚˜ì˜ algorithm ê³µìœ 
    - Heterogeneous : ê° agentê°€ ì„œë¡œ ë‹¤ë¥¸ algorithm ìš´ì˜
- model-based vs. model-free
- ë‹¤ë¥¸ agentì˜ ìƒí™©ì„ ì–¼ë§ˆë‚˜ ê³ ë ¤í•˜ëŠ”ê°€
    - none
    - actions
    - actions and rewards

### breakdown of MARL algorithms

[##_Image|kage@vz479/btr3aLfD3Mt/0geiv4790sPAxm4Oys44RK/img.png|alignCenter|width="100%"|_##]

## MARL Algorithms

<div class="aside">
<div class="title">
ğŸ“Œ stationary vs. nonstationary
</div>
<div class="contents">
stationary policy : independent of time
nonstationary policy : dependent of time
</div>
</div>

### Fully Cooperative Tasks

agentsë“¤ì´ ê³µìœ í•˜ëŠ” ê³µí†µì˜ returnì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ main goalì´ë‹¤.

- ëª¨ë“  agentë“¤ì´ ê°™ì€ reward ê°’ì„ ê³µìœ í•œë‹¤.

ì´ ê³µìœ ë˜ëŠ” ê°’ì„ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•˜ëŠëƒ ì•„ë‹ˆëƒì— ë”°ë¼ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.

<ol>
<li> controlled by centralized controller </li>
<li> indepedent agents ê°€ ë³‘ë ¬ì ìœ¼ë¡œ common optimal Q-functionì„ ì‚¬ìš© </li>
</ol>

2ë²ˆì˜ ê²½ìš° ì¶”ê°€ì ì¸ mechanisimì´ ì—†ë‹¤ë©´ ê° agentì˜ action selectionì´ optimal joint action setì„ ë§ì¹  ìˆ˜ ìˆë‹¤. (break ties randomly)

### Explicit Coordination Mechanisms

coordinated or negotiated mechanisimsë¥¼ ë„ì…í•´ [coordination problems](https://www.notion.so/A-Comprehensive-Survey-of-Multiagent-Reinforcement-Learning-6dcaaa04cbdd40d18bfef2ee7ce8afa4)ì„  í•´ê²°í•˜ê³ ì í•¨

#### How?

ëª¨ë“  agentë“¤ì´ ë™ì‹œì— â€œbreak tiesâ€ë¥¼ ì‹¤í–‰

#### ì–´ë–»ê²Œ ë™ì‹œì— ì‹¤í–‰í•˜ëŠ”ê°€?

- social conventions, roles, and communicationsë¥¼ ê¸°ë°˜ìœ¼ë¡œ
- ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œëŠ” agentì— action selectionì„ í•˜ëŠ” ìˆœì„œë¥¼ ì§€ì •í•´ì£¼ê³ , agentë§ˆë‹¤ actionì„ ê³ ë¥´ëŠ” ë°ì—ë„ ìˆœì„œë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒ
    - agent 1 â†’ 2 ìˆœì„œê°€ ë¶€ì—¬ëë‹¤ê³  ê°€ì •í•˜ì
    - agent 1ì´ ë¨¼ì € ìì‹ ì˜ action orderë¥¼ ê¸°ë°˜ìœ¼ë¡œ actionì„ ê³ ë¥´ë‚Ÿ.
    - agent 2ëŠ” agent 1ì˜ actionì„ ê¸°ë°˜ìœ¼ë¡œ ìì‹ ì˜ actionì„ ê³ ë¥¸ë‹¤.

### Fully Competitive Tasks

agentë“¤ì´ fully competitive ê´€ê³„ì¸ ê²½ìš°ëŠ” ë§Œì•½ agent 1, 2ê°€ ìˆì„ ê²½ìš° agent 1ì€ Aë¼ëŠ” taskë¥¼ ì´ë£¨ëŠ” ê²ƒì„ ëª©í‘œë¡œí•˜ê³ , agent 2ëŠ” Aë¥¼ ì´ë£¨ì§€ ì•ŠëŠ” ê²ƒì„ ëª©í‘œë¡œ í•  ê²½ìš° ë‘ agentê°€ fully competitive ê´€ê³„ì´ë‹¤.

#### minimax principle

opponent-independent algorithmsë¥¼ ì‚¬ìš©í•œë‹¤.

[##_Image|kage@b8cYmf/btr3ccp81YD/2TwGlQ09U50gvbijwV6ydK/img.png|alignCenter|width="100%"|_##]

opponentê°€ ì–´ë–¤ í–‰ë™ì„ í•˜ë“  ê´€ê³„ì—†ì´ ì–»ì„ ìˆ˜ ìˆëŠ” ìµœì†Œí•œì˜ return ê°’ ì¤‘ ìµœëŒ“ê°’ì„ ì·¨í•˜ëŠ” minimax returnì„ ì–»ê²Œ ëœë‹¤.

### Mixed Tasks

self-interested agents ë‚˜ cooperating agentsì´ë‚˜ immidiate interestsê°€ ì¶©ëŒí•˜ëŠ” ê²½ìš° ì í•©

(environmentì˜ resourceë¥¼ ê²½ìŸí•˜ëŠ” ìƒí™©)

Mixed Tasksë¥¼ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ì€ ì£¼ë¡œ <span class="text-emphasis">static tasks</span> (i.e., repeated, gerneral-sum games)ì„ ìœ„í•´ ë””ìì¸ëœ ê²ƒì´ ë§ë‹¤.

- repeated gameì—ì„œ agentì˜ dynamic behaviorë¡œ ì¸í•´ nonstationary ìƒíƒœê°€ ë˜ê¸° ë•Œë¬¸ì— <span class="text-emphasis">adaptation</span>ì´ ì¤‘ìš”!

#### 1. Single-Agent RL

- single-agent RL ì•Œê³ ë¦¬ì¦˜ì„ ë°”ë¡œ MARLì— ì ìš©
- multi agents ìƒí™©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” nonstationary ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•¨

#### 2. Agent-Independent Methods

- agentì˜ ì•Œê³ ë¦¬ì¦˜ì´ ë‹¤ë¥¸ agentsë¡œë¶€í„° ë…ë¦½ì 

[##_Image|kage@CvzCH/btr3al2Nyyf/Mz1yWiweYIreMWT186Xaw0/img.png|alignCenter|width="100%"|_##]

- $\text{solve}_i$ ëŠ” agent $i$ì˜ equilibrium ê°’ì„ ë°˜í™˜í•œë‹¤
- $\text{eval}_i$ ëŠ” agent $i$ì˜ equilibrium ê¸°ëŒ€ê°’ì„ ë°˜í™˜í•œë‹¤.
  
[##_Image|kage@uxMIw/btr3a6cSthw/PZlTuSCUHv6J3src6G0fpk/img.png|alignCenter|width="100%"|_##]

ìœ„ëŠ” Nash equilibriumìœ¼ë¡œ convergenceë¥¼ ì´ë£° ê²½ìš° $\text{solve}_i$ ì™€ $\text{eval}_i$ ì˜ˆì‹œì´ë‹¤.

- small class of problemsì—ë§Œ ì ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. (ë‹¤ë¥¸ caseì˜ ê²½ìš° ì¶”ê°€ì ì¸ mechanismì´ í•„ìš”í•˜ë‹¤

<h5>[ Example ]</h5>

- correlated equilibrium Q-learning (CE-Q)
    - correlated equilibria
- asymmetric Q-learning
    - Stackelberg (leader-follower) equilibria

#### 3. Agent-Tracking Methods

- convergence to stationary strategies ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ
- ê° agentë“¤ì€ ì„œë¡œì˜ actionì„ ëª¨ë‘ ì•Œ ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œë‹¤.

<h5> Static Tasks</h5>

- fictitious plays
- MetaStrategy
- Hyper-Q

<h5>Dynamic Tasks</h5>
    
- Nonstationary Convergin Policies (NSCP)
- computes a best response

[##_Image|kage@btDDpp/btr3dbEhBF3/Cqvz8URr9b8ZP5oRP1dqYK/img.png|alignCenter|width="100%"|_##]

#### 4. Agent-Aware Methods

- convergence, adaptation ëª¨ë‘ ë…¸ë¦¼

#### 5. Remarks and Open Issues

- dynamic tasksë¡œì˜ í™•ì¥
- exact task modelì„ ê¸°ë°˜ìœ¼ë¡œ í•¨ (í˜„ì‹¤ì„± â¬‡ï¸)
- ì°¨ì›ì˜ ì €ì£¼ â†’ ë‹¤ë¥¸ agentë„ observation í•´ policyì— ë°˜ì˜í•˜ê¸° ë•Œë¬¸
- í™•ì‹¤í•˜ì§€ ì•Šì€ observation


## Multi-Agents in Network System

ë„¤íŠ¸ì›Œí¬ ì‹œìŠ¤í…œì—ì„œ ë…¸ë“œë“¤ì—ê²Œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ìì‹ ì˜ íŒ¨í‚·ì„ ì‹¤íŒ¨í•˜ì§€ ì•Šê³  ë³´ë‚´ëŠ” ê²ƒì´ê³ , ìì‹ ì´ íŒ¨í‚·ì„ ë³´ë‚¼ ê¸°íšŒë¥¼ ì¡ëŠ” ê²ƒì´ë‹¤.

â‡’ ì¦‰, agent ìì‹ ì´ throughputì„ ì˜¬ë¦¬ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.
  
<br/>

í•˜ì§€ë§Œ ì‹œìŠ¤í…œ ì „ì²´ì—ì„œëŠ” resourceê°€ ë‚­ë¹„ë˜ì§€ ì•ŠëŠ” ê²ƒ, ê·¸ë¦¬ê³  edge nodesì—ê²Œë„ ì „ì†¡ì˜ ê¸°íšŒë¥¼ ì£¼ëŠ” ê²ƒì´ ëª©í‘œë¼ê³  í•  ìˆ˜ ìˆë‹¤. 

<br/>
  
agentì™€ ì‹œìŠ¤í…œ ì „ì²´ì˜ ëª©í‘œê°€ ê°™ìœ¼ë©´ì„œë„ ë‹¨ìˆœíˆ throughputë§Œ ë†’ì•„ì„  ì•ˆë˜ê¸° ë•Œë¬¸ì— ì„œë¡œ ìƒì¶©í•˜ëŠ” ë¶€ë¶„ì´ ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

<br/>
  
ë˜í•œ, agentë“¤ì€ ìì‹ ì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ resourceë¥¼ ê³µìœ í•œë‹¤. ì¦‰, ê²½ìŸ ê´€ê³„ì— ìˆëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì‹œìŠ¤í…œì˜ ëª©í‘œë¥¼ ì´ë£¨ê¸° ìœ„í•´ì„œëŠ” edge nodesì— ì–´ëŠ ì •ë„ ì–‘ë³´ê°€ ì´ë£¨ì–´ì ¸ì•¼ í•˜ê¸° ë•Œë¬¸ì— í˜‘ë ¥ì€ ë¶ˆê°€í”¼í•˜ë‹¤. ê¸°ì¡´ í”„ë¡œí† ì½œì€ ìš°ìœ„ ë…¸ë“œë“¤ì— íŒ¨í‚· ì „ì†¡ ê¸°íšŒê°€ ì ë¦´ ìˆ˜ ë°–ì— êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì´ë‹¤. (íŠ¹íˆ doubling contention windowì—ì„œ)
  
<br/>

ë”°ë¼ì„œ stabilityì™€ adaptationì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•˜ê³ , stabilityì˜ ê²½ìš° ì–´ëŠ ì •ë„ ê· í˜• ìƒíƒœ (edge nodesì— ìµœì†Œí•œì˜ throughputì„ ë³´ì¥) í•˜ë©´ì„œë„ ìš°ìœ„ ë…¸ë“œë“¤ì€ ìì‹ ì˜ throughputì„ ìµœëŒ€í™”í•  ìˆ˜ ìˆëŠ” convergene ìƒíƒœë¥¼ ì°¾ì•„ì•¼ í•œë‹¤. (ì „ì²´ ì‹œìŠ¤í…œ throughputì´ ë„ˆë¬´ ë‚´ë ¤ê°€ì§€ ì•Šë„ë¡) adaptiationì˜ ê²½ìš° edge nodesë“¤ì´ ìµœì†Œí•œì˜ throughput ë°‘ìœ¼ë¡œ ë‚´ë ¤ê°€ì§€ ì•ŠëŠ” â€œno-regretâ€ ìƒíƒœë¥¼ ìœ ì§€í•´ì•¼í•œë‹¤.

## Further Studies

 ğŸŠ ì¶”ê°€ ë¬¸ì„œë“¤ 

- MARL ê´€ë ¨ gitbook : [https://kilmya1.gitbook.io/deep-multi-agent-reinforcement-learning/2.-background/2.-background/2.2-multi-agent-settings](https://kilmya1.gitbook.io/deep-multi-agent-reinforcement-learning/2.-background/2.-background/2.2-multi-agent-settings)
- **ê²°í•©í™•ë¥ ë¶„í¬(Joint Distributions) :** [https://everyday-image-processing.tistory.com/23](https://everyday-image-processing.tistory.com/23)